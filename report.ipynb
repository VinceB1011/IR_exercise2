{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a230da76",
   "metadata": {},
   "source": [
    "## Report Milestone 2 Exercise 2 Information Retrieval 2026\n",
    "This report documents how our group (group 40) completed the second milestone of exercise 2. In general we were tasked to improve the approach used in milestone 1. Where we had to experiment with different configurations for Audio Fingerprinting and Identifiction. The improvments are based on the Fingerprinting-algorithm by Wang (2003).\n",
    "The goal of milestone 2.2 was to develop a robust pipeline that accurately identifies music tracks even under difficult conditions (noise, cell phone recordings). A particular focus was placed on the efficient storage of fingerprints in an inverted index and scaling the system to a dataset of over 3,000 tracks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28408894",
   "metadata": {},
   "source": [
    "### Task 1: Hash Generation and Storage (30%)\n",
    "\n",
    "The optimal parameters from milestone 2.1 (k=18, t=15) were used to extract the distinctive points (peaks) from the spectrogram. These parameters define the size of the local neighborhood filter and ensure that only the most significant frequency peaks are selected as anchor points. To increase robustness against temporal and frequency-related distortions, we tested various configurations for the target zone. \n",
    "\n",
    "| Configuration | min time | max time | max frequency | Fan-out | Description |\n",
    "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
    "| `std` | 10 | 50 | 50 | 3 | basic config|\n",
    "| `wide_t` | 10 | **100** | 50 | 3 | focus on bigger time window |\n",
    "| `wide_f` | 10 | 50 | **100** | 3 | focus on bigger frequency window |\n",
    "| `dense` | 10 | 50 | 50 | **6** | more pairs through increased fanout |\n",
    "| `super_dense` | 10 | **60** | **100** | **15** | even more increased fanout |\n",
    "\n",
    "To test all those configuration we implemented a improved script that supports capturing the tracks as hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a449c",
   "metadata": {},
   "source": [
    "#### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2669507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import sys\n",
    "import pickle\n",
    "from utils import get_peaks, generate_hashes_from_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2276321",
   "metadata": {},
   "source": [
    "#### define configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e111e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"std\", 10, 50, 50, 3),      # basic config\n",
    "    (\"wide_t\", 10, 100, 50, 3),   # focus on bigger time window\n",
    "    (\"wide_f\", 10, 50, 100, 3),   # focus on bigger frequency window\n",
    "    (\"dense\", 10, 50, 50, 6),      # more pairs through increased fanout\n",
    "    (\"super_dense\", 10, 60, 100, 15) # even more increased fanout\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cdf55",
   "metadata": {},
   "source": [
    "#### get_peak and generate_hashes_from_peaks functions\n",
    "\n",
    "### 32-Bit Hash Konstruktion\n",
    "\n",
    "A key aspect of Wang's algorithm is the representation of pairs (anchor and target points) as compact 32-bit unsigned integers.  \n",
    " We have opted for the following bit allocation to achieve an optimal balance between precision and value range:  \n",
    " * 10 bits for $f_1$ (anchor frequency): Allows 1024 frequency bins, which covers the full resolution of our STFT spectrogram.   \n",
    " * 10 bits for $f_2$ (target frequency): Also 1024 bins for the target point.  \n",
    " * 12 bits for $\\Delta t$ (time difference): Allows a maximum difference of 4096 frames. With a hop size of 1024, this corresponds to approximately 190 seconds – far more than our maximum target zone width of 60 frames, giving us buffer for future expansions.  \n",
    " \n",
    " Hash formula: `hash_32 = (f1 | (f2 << 10) | (dt << 20)) & 0xFFFFFFFF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be5b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peaks(Y, k, t, thresh=0.01):\n",
    "    size = (2*k + 1, 2*t + 1)\n",
    "    result = ndimage.maximum_filter(Y, size=size, mode='constant')\n",
    "    cmap = (Y == result) & (Y > thresh)\n",
    "    return np.argwhere(cmap)\n",
    "\n",
    "def generate_hashes_from_peaks(peaks, dt_min, dt_max, df_max, fan_out):\n",
    "    # Sort peaks by time (column 1)\n",
    "    peaks = peaks[peaks[:, 1].argsort()]\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(len(peaks)):\n",
    "        anchor = peaks[i]\n",
    "        f1, t1 = anchor[0], anchor[1]\n",
    "        \n",
    "        # Look for targets in a window after the anchor\n",
    "        count = 0\n",
    "        for j in range(i + 1, len(peaks)):\n",
    "            target = peaks[j]\n",
    "            f2, t2 = target[0], target[1]\n",
    "            dt = t2 - t1\n",
    "            \n",
    "            # Check if target is within the Target Zone\n",
    "            if dt_min <= dt <= dt_max and abs(f2 - f1) <= df_max:\n",
    "                # 1. Create the 32-bit Hash\n",
    "                # Example: pack f1 (10 bits), f2 (10 bits), and dt (12 bits)\n",
    "\n",
    "                hash_32 = (f1 | (f2 << 10) | (dt << 20)) & 0xFFFFFFFF\n",
    "                \n",
    "                # 2. Store (hash, t1)\n",
    "                pairs.append((hash_32, t1))\n",
    "                \n",
    "                count += 1\n",
    "                if count >= fan_out:\n",
    "                    break\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7ea51",
   "metadata": {},
   "source": [
    "#### iterrate over all songs and process them and save in inverted index as pickle file\n",
    "\n",
    "We utilized Python dictionaries, where the keys are the generated 32-bit hashes and the values are he Key is the unique 32-bit hash nd the Value is a list containing multiple tuples. Each tuple represents an individual 'hit' or occurrence, allowing the system to track every song and every timestamp where that specific hash appears. Each uple contains (song_id, offset_time).\n",
    "\n",
    " To maximize efficiency, a separate Song Map stores to store the specic song names, ensuring the main index remains lean by only holding integer-based pointers. Unlike linear arrays, which require slow step-by-step searching, or SQL databases that carry high processing overhead, the dictionary approach handles the millions of entries in Task 3 with minimal latency. This architecture ensures that even with 3,319 tracks, the system can jump directly to relevant candidates during the retrieval phase.\n",
    "\n",
    " To preserve the inverted index for large-scale use, we serialized the Python dictionary into Pickle files, which allow for the direct saving and loading of complex data structures without manual conversion. Unlike plain text or CSV formats, Pickle maintains the specific hierarchy of our \"key-to-list\" mapping, ensuring the dictionary is ready for immediate use upon loading. We specifically prioritized Standard Pickle (Protocol 4/5) over compressed formats like bz2 to maximize I/O speed, significantly reducing the time required to save millions of hashes during the Task 3 scale-up. This approach was essential for the 3,319-track dataset, as it allowed the desktop hardware to handle large write operations without the CPU bottlenecks associated with heavy compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6a8331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting indexing for all configurations...\n",
      " > Config std finished and saved.\n",
      " > Config wide_t finished and saved.\n",
      " > Config wide_f finished and saved.\n",
      " > Config dense finished and saved.\n",
      " > Config super_dense finished and saved.\n",
      "\n",
      "================================================================================\n",
      "CONFIG          | UNIQUE HASHES   | TOTAL ENTRIES   | AVG/TRACK    | SIZE (MB) \n",
      "--------------------------------------------------------------------------------\n",
      "std             | 2,184,674       | 5,341,826       | 9306.3       | 80.00     \n",
      "wide_t          | 2,897,206       | 6,369,372       | 11096.5      | 160.00    \n",
      "wide_f          | 3,007,011       | 6,236,003       | 10864.1      | 160.00    \n",
      "dense           | 2,442,910       | 6,614,762       | 11524.0      | 80.00     \n",
      "super_dense     | 6,159,719       | 16,258,554      | 28325.0      | 320.00    \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def run_indexing(config_name, dt_min, dt_max, df_max, fan_out):\n",
    "    audio_folder = 'data/40'\n",
    "    song_files = [f for f in os.listdir(audio_folder) if f.endswith('.mp3')]\n",
    "    inverted_index = {}\n",
    "    song_map = {}\n",
    "\n",
    "    for song_id, song in enumerate(song_files):\n",
    "        song_map[song_id] = song\n",
    "        path = os.path.join(audio_folder, song)\n",
    "        \n",
    "        # load songs (full length)\n",
    "        y, sr = librosa.load(path, sr=22050)\n",
    "        Y = np.abs(librosa.stft(y, n_fft=2048, hop_length=1024))\n",
    "        \n",
    "        peaks = get_peaks(Y, 18, 15)\n",
    "        pairs = generate_hashes_from_peaks(peaks, dt_min, dt_max, df_max, fan_out)\n",
    "\n",
    "        for h, t1 in pairs:\n",
    "            if h not in inverted_index:\n",
    "                inverted_index[h] = []\n",
    "            inverted_index[h].append((song_id, t1))\n",
    "\n",
    "    # calculate some statistics\n",
    "    total_unique = len(inverted_index)\n",
    "    total_entries = sum(len(v) for v in inverted_index.values())\n",
    "    avg_per_track = total_entries / len(song_files) if song_files else 0\n",
    "    size_mb = sys.getsizeof(inverted_index) / (1024**2)\n",
    "\n",
    "    # save as pickle file\n",
    "    filename = f\"db_{config_name}.pkl\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({'index': inverted_index, 'map': song_map}, f)\n",
    "    \n",
    "    print(f\" > Config {config_name} finished and saved.\")\n",
    "\n",
    "    # return stats\n",
    "    return {\n",
    "        \"name\": config_name,\n",
    "        \"unique\": total_unique,\n",
    "        \"total\": total_entries,\n",
    "        \"avg\": avg_per_track,\n",
    "        \"size\": size_mb\n",
    "    }\n",
    "\n",
    "def task1():\n",
    "    all_stats = []\n",
    "    \n",
    "    print(\"Starting indexing for all configurations...\")\n",
    "    for name, dt_min, dt_max, df_max, fan_out in configs:\n",
    "        stats = run_indexing(name, dt_min, dt_max, df_max, fan_out)\n",
    "        all_stats.append(stats)\n",
    "\n",
    "    # final summary for stats\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{'CONFIG':<15} | {'UNIQUE HASHES':<15} | {'TOTAL ENTRIES':<15} | {'AVG/TRACK':<12} | {'SIZE (MB)':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for s in all_stats:\n",
    "        print(f\"{s['name']:<15} | {s['unique']:<15,} | {s['total']:<15,} | {s['avg']:<12.1f} | {s['size']:<10.2f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "task1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51cd6b",
   "metadata": {},
   "source": [
    "The evaluation of five distinct configurations revealed a clear trade-off between fingerprint density and resource consumption. While the std configuration was the most lightweight with ~9,306 entries per track, the super_dense configuration increased this density by over 300%, resulting in a massive 16.2 million total entries. The avg/track category informs how many hash entries where saved in the dictionary on average.\n",
    "\n",
    " * Density vs. Size: As density increased, the storage footprint grew proportionally, peaking at 320MB for the super_dense model.\n",
    "\n",
    " * Search Robustness: The wide_t and wide_f configurations increased the \"Target Zone,\" which produced more unique hashes and improved the system's ability to handle time-stretching or frequency shifts.\n",
    "\n",
    " * Optimal Balance: The dense configuration provided a significant boost in entries (11,524 per track) while maintaining the same 80MB storage footprint as the standard model, demonstrating high indexing efficiency.\n",
    "\n",
    " * Scalability Insight: These results directly informed the choice of the smart_dense configuration for Task 3, ensuring enough redundancy for a 75% mobile-query accuracy (see task 2) without exceeding storage capabilities of our desktop hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e63487",
   "metadata": {},
   "source": [
    "## Task 2: Audio Identification (30%)\n",
    "\n",
    "In Milestone 2.1, audio identification was performed by directly matching\n",
    "constellation maps.\n",
    "In this task, we instead use **fingerprint hashes** generated from peak pairs.\n",
    "\n",
    "Each hash represents:\n",
    "- Two frequency bins\n",
    "- Their time difference\n",
    "\n",
    "Hashes are stored in an inverted index and matched using\n",
    "**temporal offset consistency voting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "887eb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import glob\n",
    "import csv\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eace1a",
   "metadata": {},
   "source": [
    "## Paths and Database Configuration\n",
    "\n",
    "The fingerprint databases were generated in Task 1 and stored as pickle files.\n",
    "Each database corresponds to a different fingerprint configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "671257df",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ROOT = \"queries\"\n",
    "OUT_DIR = \"ms22_task2_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DB_PKLS = {\n",
    "    \"std\": \"db_std.pkl\",\n",
    "    \"wide_t\": \"db_wide_t.pkl\",\n",
    "    \"wide_f\": \"db_wide_f.pkl\",\n",
    "    \"dense\": \"db_dense.pkl\",\n",
    "    \"super_dense\": \"db_super_dense.pkl\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ad793",
   "metadata": {},
   "source": [
    "## Audio and Peak Detection Parameters\n",
    "\n",
    "All parameters must match those used during fingerprint indexing (Task 1).\n",
    "Peak detection parameters are taken from the best-performing configuration\n",
    "identified in Milestone 2.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2503412",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP = 1024\n",
    "BIN_MAX = 128\n",
    "\n",
    "Q_DURATION = 10.0\n",
    "\n",
    "PEAK_K = 18\n",
    "PEAK_T = 15\n",
    "PEAK_THRESH = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4b6e5",
   "metadata": {},
   "source": [
    "## Query Sets\n",
    "\n",
    "Queries are grouped by distortion type:\n",
    "- Original\n",
    "- Noise\n",
    "- Coding\n",
    "- Mobile\n",
    "\n",
    "These match the setup used in Milestone 2.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d3823a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_SUBFOLDERS = {\n",
    "    \"Original\": \"Original\",\n",
    "    \"Noise\": \"Noise\",\n",
    "    \"Coding\": \"Coding\",\n",
    "    \"Mobile\": \"Mobile\",\n",
    "}\n",
    "\n",
    "def collect_queries(query_root: str) -> Dict[str, List[str]]:\n",
    "    out = {}\n",
    "    for dtype, sub in QUERY_SUBFOLDERS.items():\n",
    "        folder = os.path.join(query_root, sub)\n",
    "        ext = \"*.wav\" if dtype != \"Coding\" else \"*.mp3\"\n",
    "        out[dtype] = sorted(glob.glob(os.path.join(folder, ext)))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d98c8",
   "metadata": {},
   "source": [
    "## Ground Truth Extraction\n",
    "\n",
    "The ground truth track ID is extracted from the query filename.\n",
    "A query is counted as correct if the predicted filename matches the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad60e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_track_id(path: str) -> Optional[str]:\n",
    "    base = os.path.basename(path)\n",
    "    m = re.search(r\"(\\d+)\", base)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def gt_db_filename_from_id(track_id: str) -> str:\n",
    "    return f\"{track_id}.mp3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859f67f",
   "metadata": {},
   "source": [
    "## Hash Database Structure\n",
    "\n",
    "Each database pickle contains:\n",
    "- `index`: dictionary mapping hash → list of (song_id, time_offset)\n",
    "- `map`: dictionary mapping song_id → filename\n",
    "\n",
    "This corresponds to an inverted index as described by Müller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9083e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hash_db(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj[\"index\"], obj[\"map\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434377a1",
   "metadata": {},
   "source": [
    "## Hash-Based Matching Strategy\n",
    "\n",
    "For each query:\n",
    "1. Compute STFT magnitude\n",
    "2. Detect spectral peaks\n",
    "3. Generate fingerprint hashes\n",
    "4. Look up hashes in the database\n",
    "5. Vote for (song_id, Δt) pairs\n",
    "6. Select the song with the highest consistent offset vote\n",
    "\n",
    "This is the classical **Wang offset-voting scheme**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23bbe4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_query_hash(\n",
    "    query_path,\n",
    "    index,\n",
    "    song_map,\n",
    "    dt_min,\n",
    "    dt_max,\n",
    "    df_max,\n",
    "    fan_out\n",
    "):\n",
    "    y, _ = librosa.load(query_path, sr=SR, mono=True, duration=Q_DURATION)\n",
    "    Y = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP))\n",
    "\n",
    "    peaks = get_peaks(Y, PEAK_K, PEAK_T, PEAK_THRESH)\n",
    "    pairs = generate_hashes_from_peaks(peaks, dt_min, dt_max, df_max, fan_out)\n",
    "\n",
    "    votes = Counter()\n",
    "\n",
    "    for h, t_q in pairs:\n",
    "        if h not in index:\n",
    "            continue\n",
    "        for song_id, t_db in index[h]:\n",
    "            votes[(song_id, t_db - t_q)] += 1\n",
    "\n",
    "    if not votes:\n",
    "        return \"\", 0\n",
    "\n",
    "    best_song = max(\n",
    "        votes,\n",
    "        key=lambda x: votes[x]\n",
    "    )[0]\n",
    "\n",
    "    return song_map[best_song], max(votes.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e9027",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "For each configuration and distortion type we measure:\n",
    "- Number of correct identifications (hits)\n",
    "- Accuracy\n",
    "- Average query time\n",
    "- Average maximum offset vote\n",
    "\n",
    "Results are stored for later analysis and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d2fb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Row:\n",
    "    distortion: str\n",
    "    config: str\n",
    "    dt_min: int\n",
    "    dt_max: int\n",
    "    df_max: int\n",
    "    fan_out: int\n",
    "    hits: int\n",
    "    total: int\n",
    "    accuracy: float\n",
    "    avg_query_time_s: float\n",
    "    avg_best_vote: float\n",
    "\n",
    "def write_csv(path: str, rows: List[Row]):\n",
    "    if not rows:\n",
    "        return\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(asdict(rows[0]).keys()))\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(asdict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6195f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 20 queries found\n",
      "Noise: 20 queries found\n",
      "Coding: 20 queries found\n",
      "Mobile: 20 queries found\n",
      "\n",
      "=== HASH CONFIG std (db_std.pkl) ===\n",
      "Loaded index: 2184674 unique hashes, 574 songs\n",
      "  => Original: hits 20/20, acc=1.00, avg_time=0.03s, avg_vote=112.8\n",
      "  => Noise: hits 20/20, acc=1.00, avg_time=0.03s, avg_vote=55.4\n",
      "  => Coding: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=47.4\n",
      "  => Mobile: hits 13/20, acc=0.65, avg_time=0.01s, avg_vote=11.3\n",
      "  => Combined: hits 73/80, acc=0.91, avg_time=0.04s, avg_vote=56.7\n",
      "\n",
      "=== HASH CONFIG wide_t (db_wide_t.pkl) ===\n",
      "Loaded index: 2897206 unique hashes, 574 songs\n",
      "  => Original: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=127.5\n",
      "  => Noise: hits 20/20, acc=1.00, avg_time=0.03s, avg_vote=63.6\n",
      "  => Coding: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=54.4\n",
      "  => Mobile: hits 14/20, acc=0.70, avg_time=0.01s, avg_vote=12.1\n",
      "  => Combined: hits 74/80, acc=0.93, avg_time=0.02s, avg_vote=64.4\n",
      "\n",
      "=== HASH CONFIG wide_f (db_wide_f.pkl) ===\n",
      "Loaded index: 3007011 unique hashes, 574 songs\n",
      "  => Original: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=123.1\n",
      "  => Noise: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=60.2\n",
      "  => Coding: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=52.0\n",
      "  => Mobile: hits 14/20, acc=0.70, avg_time=0.01s, avg_vote=11.2\n",
      "  => Combined: hits 74/80, acc=0.93, avg_time=0.02s, avg_vote=61.6\n",
      "\n",
      "=== HASH CONFIG dense (db_dense.pkl) ===\n",
      "Loaded index: 2442910 unique hashes, 574 songs\n",
      "  => Original: hits 20/20, acc=1.00, avg_time=0.03s, avg_vote=139.1\n",
      "  => Noise: hits 20/20, acc=1.00, avg_time=0.03s, avg_vote=68.5\n",
      "  => Coding: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=58.2\n",
      "  => Mobile: hits 13/20, acc=0.65, avg_time=0.02s, avg_vote=14.4\n",
      "  => Combined: hits 73/80, acc=0.91, avg_time=0.02s, avg_vote=70.0\n",
      "\n",
      "=== HASH CONFIG super_dense (db_super_dense.pkl) ===\n",
      "Loaded index: 6159719 unique hashes, 574 songs\n",
      "  => Original: hits 20/20, acc=1.00, avg_time=0.19s, avg_vote=324.9\n",
      "  => Noise: hits 20/20, acc=1.00, avg_time=0.04s, avg_vote=166.3\n",
      "  => Coding: hits 20/20, acc=1.00, avg_time=0.02s, avg_vote=139.1\n",
      "  => Mobile: hits 15/20, acc=0.75, avg_time=0.02s, avg_vote=34.0\n",
      "  => Combined: hits 75/80, acc=0.94, avg_time=0.03s, avg_vote=166.1\n",
      "\n",
      "Done. CSVs written to: ms22_task2_results\n"
     ]
    }
   ],
   "source": [
    "def task2():\n",
    "    queries = collect_queries(QUERY_ROOT)\n",
    "    for dtype, qfiles in queries.items():\n",
    "        print(f\"{dtype}: {len(qfiles)} queries found\", flush=True)\n",
    "\n",
    "    # Same 4 configs as your colleague used for indexing:\n",
    "    # (name, dt_min, dt_max, df_max, fan_out)\n",
    "    CONFIGS = [\n",
    "        (\"std\",    10,  50,  50, 3),\n",
    "        (\"wide_t\", 10, 100,  50, 3),\n",
    "        (\"wide_f\", 10,  50, 100, 3),\n",
    "        (\"dense\",  10,  50,  50, 6),\n",
    "        (\"super_dense\", 10, 60, 100, 15)\n",
    "    ]\n",
    "\n",
    "    all_rows: List[Row] = []\n",
    "\n",
    "    # also create a combined pool of queries\n",
    "    combined_queries: List[Tuple[str, str]] = []\n",
    "    for dist, files in queries.items():\n",
    "        for p in files:\n",
    "            combined_queries.append((dist, p))\n",
    "\n",
    "    for cfg_name, dt_min, dt_max, df_max, fan_out in CONFIGS:\n",
    "        db_path = DB_PKLS[cfg_name]\n",
    "        print(f\"\\n=== HASH CONFIG {cfg_name} ({db_path}) ===\", flush=True)\n",
    "\n",
    "        index, song_map = load_hash_db(db_path)\n",
    "        print(f\"Loaded index: {len(index)} unique hashes, {len(song_map)} songs\", flush=True)\n",
    "        # per distortion\n",
    "        for distortion, qfiles in queries.items():\n",
    "            if not qfiles:\n",
    "                continue\n",
    "\n",
    "            hits = 0\n",
    "            times = []\n",
    "            votes = []\n",
    "            total = len(qfiles)\n",
    "\n",
    "            for qpath in qfiles:\n",
    "                tid = extract_track_id(qpath)\n",
    "                if tid is None:\n",
    "                    continue\n",
    "                gt = gt_db_filename_from_id(tid)\n",
    "\n",
    "                t0 = time.time()\n",
    "                pred, best_vote = identify_query_hash(\n",
    "                    query_path=qpath,\n",
    "                    index=index,\n",
    "                    song_map=song_map,\n",
    "                    dt_min=dt_min,\n",
    "                    dt_max=dt_max,\n",
    "                    df_max=df_max,\n",
    "                    fan_out=fan_out,\n",
    "                )\n",
    "                dt = time.time() - t0\n",
    "                times.append(dt)\n",
    "                votes.append(best_vote)\n",
    "\n",
    "                if pred == gt:\n",
    "                    hits += 1\n",
    "\n",
    "            avg_t = float(np.mean(times)) if times else 0.0\n",
    "            avg_vote = float(np.mean(votes)) if votes else 0.0\n",
    "\n",
    "            row = Row(\n",
    "                distortion=distortion,\n",
    "                config=cfg_name,\n",
    "                dt_min=dt_min,\n",
    "                dt_max=dt_max,\n",
    "                df_max=df_max,\n",
    "                fan_out=fan_out,\n",
    "                hits=hits,\n",
    "                total=total,\n",
    "                accuracy=hits / total if total else 0.0,\n",
    "                avg_query_time_s=avg_t,\n",
    "                avg_best_vote=avg_vote,\n",
    "            )\n",
    "            all_rows.append(row)\n",
    "            print(f\"  => {distortion}: hits {hits}/{total}, acc={row.accuracy:.2f}, avg_time={avg_t:.2f}s, avg_vote={avg_vote:.1f}\", flush=True)\n",
    "\n",
    "        # combined\n",
    "        hits = 0\n",
    "        times = []\n",
    "        votes = []\n",
    "        total = len(combined_queries)\n",
    "\n",
    "        for distortion, qpath in combined_queries:\n",
    "            tid = extract_track_id(qpath)\n",
    "            if tid is None:\n",
    "                continue\n",
    "            gt = gt_db_filename_from_id(tid)\n",
    "\n",
    "            t0 = time.time()\n",
    "            pred, best_vote = identify_query_hash(\n",
    "                query_path=qpath,\n",
    "                index=index,\n",
    "                song_map=song_map,\n",
    "                dt_min=dt_min,\n",
    "                dt_max=dt_max,\n",
    "                df_max=df_max,\n",
    "                fan_out=fan_out,\n",
    "            )\n",
    "            dt = time.time() - t0\n",
    "            times.append(dt)\n",
    "            votes.append(best_vote)\n",
    "            if pred == gt:\n",
    "                hits += 1\n",
    "\n",
    "        avg_t = float(np.mean(times)) if times else 0.0\n",
    "        avg_vote = float(np.mean(votes)) if votes else 0.0\n",
    "\n",
    "        all_rows.append(Row(\n",
    "            distortion=\"Combined\",\n",
    "            config=cfg_name,\n",
    "            dt_min=dt_min,\n",
    "            dt_max=dt_max,\n",
    "            df_max=df_max,\n",
    "            fan_out=fan_out,\n",
    "            hits=hits,\n",
    "            total=total,\n",
    "            accuracy=hits / total if total else 0.0,\n",
    "            avg_query_time_s=avg_t,\n",
    "            avg_best_vote=avg_vote,\n",
    "        ))\n",
    "        print(f\"  => Combined: hits {hits}/{total}, acc={hits/total:.2f}, avg_time={avg_t:.2f}s, avg_vote={avg_vote:.1f}\", flush=True)\n",
    "\n",
    "    write_csv(os.path.join(OUT_DIR, \"all_results.csv\"), all_rows)\n",
    "\n",
    "    per_distortion = defaultdict(list)\n",
    "    for r in all_rows:\n",
    "        per_distortion[r.distortion].append(r)\n",
    "\n",
    "    for distortion, rows in per_distortion.items():\n",
    "        fname = f\"{distortion.lower()}_table.csv\"\n",
    "        write_csv(os.path.join(OUT_DIR, fname), rows)\n",
    "\n",
    "    print(\"\\nDone. CSVs written to:\", OUT_DIR, flush=True)\n",
    "\n",
    "task2() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cd23d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The evaluation results from Task 2 provided the empirical evidence needed to finalize our strategy for the 3,319-track scale-up. Across all configurations, Clean, Noise, and Coding queries achieved a perfect 1.00 accuracy, demonstrating that the core hashing logic is extremely resilient to digital distortions. The true differentiator was the Mobile category, where accuracy scaled from 0.65 (std) to 0.75 (super_dense).\n",
    "\n",
    "While super_dense offered the highest accuracy and the strongest \"confidence\" (indicated by an avg_vote of 34.0 for mobile queries), it came at a significant computational cost, with a lookup time of 0.19s. Nearly six times slower than the standard model. We observed that increasing the \"Target Zone\" (the wide_t and wide_f configs) consistently improved mobile performance to 0.70 without the massive memory bloat seen in super_dense.\n",
    "\n",
    "These findings led us to design the smart_dense configuration for Task 3. By combining the increased time/frequency ranges of the \"wide\" configurations with a moderate increase in \"fan-out\" density (fanout = 8), we aimed to capture the 0.75 accuracy of the super-dense model while keeping the indexing time and RAM usage within our hrdware limits. Essentially, smart_dense was engineered to maximize the \"votes\" for noisy queries while maintaining the sub-0.05s search speeds observed in our more efficient test runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d812188",
   "metadata": {},
   "source": [
    "## Task 3: Scale-Up (30%)\n",
    "\n",
    "In Task 2, we tested different hashing configurations on **30-second track snippets**.\n",
    "The best-performing configuration, **`super_dense`**, uses a larger frequency window and increased fan_out.\n",
    "\n",
    "The final **`super_dense`** configuration looks as follows: \n",
    "- dt_min: 10\n",
    "- dt_max: 60\n",
    "- df_max: 100\n",
    "- fanout: 8\n",
    "\n",
    "\n",
    "The pyhton script to generate the whole scaled up database is provided in the gen_final_database.py file in the ZIP.  \n",
    "\n",
    "In this notebook, we scale this configuration to:\n",
    "\n",
    "- Index **full-length tracks**.\n",
    "- Include six tarballs from the MTG-Jamendo dataset.\n",
    "- Collect statistics on database size, total hashes, and build time.\n",
    "- Save a final index and CSV with results.\n",
    "\n",
    "We will leverage **checkpoint files** to avoid recomputing the full database, since building the index from scratch is time-consuming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138db79",
   "metadata": {},
   "source": [
    "### Task 3: Large-Scale Indexing Results (MTG-Jamendo Dataset)\n",
    "\n",
    "The final scale-up utilized the **smart_dense** configuration on a high-performance desktop environment to index the full 3,319-track dataset. The results demonstrate the system's ability to maintain high fingerprint density while remaining within hardware constraints.\n",
    "\n",
    "### Key Performance Metrics\n",
    "| Metric | Value |\n",
    "| :--- | :--- |\n",
    "| **Total Tracks Indexed** | 3,319 |\n",
    "| **Total Hash Entries** | 83,636,108 |\n",
    "| **Unique Hash Keys** | 9,198,074 |\n",
    "| **Avg. Hashes per Track** | ~25,199 |\n",
    "| **Index Storage Size** | ~1,1 GB|\n",
    "| **Total Build Time** | 18,195.11 s (~5.05 hours) |\n",
    "\n",
    "### Results Analysis\n",
    "* **Hash Density:** By achieving **~25,199 hashes per track**, the system maintains a redundancy level similar to the *super_dense* configuration from Task 2, which is critical for robust matching in noisy environments.\n",
    "* **Hash Collisions:** While there are over 83 million total entries, they are mapped to only 9.2 million unique keys. This shows that many audio patterns are repeated across the dataset, justifying the use of an **Inverted Index** to group these occurrences efficiently.\n",
    "* **Build Throughput:** The indexing rate averaged approximately **5.48 seconds per song**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
